<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Luke Marris' personal wesbite.">
    <meta name="keywords" content="Luke Marris, N-player, General-Sum, Solution Concepts, Equilibrium Selection, Nash Equilibrium, NE, Correlated Equilibrium, CE, Coarse Correlated Equilibrium, CCE, DeepMind, Cambridge, UCL, Jesus College, Reinforcement Learning, RL, Multiagent, MARL, PSRO, JPSRO, MGCE, Payoff Rating, Elo, Nash Average, Game Theory, Multiagent Systems, ICML, NeurIPS, ICLR">
    <meta name="author" content="Luke Marris">
    <title>Luke Marris</title>

    <!-- Bootstrap core CSS -->
    <link
        href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css"
        rel="stylesheet"
        integrity="sha384-Zenh87qX5JnK2Jl0vWa8Ck2rdkQ2Bzep5IDxbcnCeuOxjzrPF/et3URy9Bv1WTRi"
        crossorigin="anonymous">

    <!-- Favicons -->
    <link rel="icon" href="favicon.ico">
    
    <!-- Privacy Friendly Analytics -->
    <script src="https://cdn.counter.dev/script.js" data-id="08224b8c-5ef2-4b5a-a31e-342a17c8f243" data-utcoffset="1"></script>
    
    <!-- Custom Stylesheet -->
    <style>
      @media (min-width: 1200px) {
          .container{
              max-width: 970px;
          }
      }
      
      .background-image {
          background-image: url(marris_background_frame.jpg);
          background-repeat: no-repeat;
          background-position: center;
      }

      .big-bullet li::marker {
          font-size: 2rem;
      }
    </style>
    
  </head>
  <body class="d-flex flex-column h-100">

    <!-- Begin page content -->
    <header class="flex-shrink-0 bg-light py-5 mb-5 background-image">
      <div class="container text-center py-5">
        <img src="marris_portrait.jpeg" class="rounded mx-auto d-block" alt="Luke Marris portrait" width="360" height="360">
      </div>
    </header>
    
    <main class="flex-shrink-0">
      <section class="container py-2">
        <h1 class="font-weight-bold display-1">Dr Luke Marris</h1>
        <p class="lead">
          I am an artificial intelligence engineer and researcher. I have expertise in machine learning, optimization, deep learning, reinforcement learning, game theory and multiagent systems.
          In particular, I am interested in training deep reinforcement agents, at scale, in many-player mixed-motive games, with a focus on building principled learning algorthims that provably select and compute equilibria.
          Games are more than activities we play with our friends: any interaction between multiple self-interested players can be modelled as a game.
          Markets, trade negotiations, social norms, defence policy, environmental treaties, auctions, animal behaviour, and coordinated pandemic response, are all complex games.
          Understanding and calculating fair, stable, and rewarding solutions to games is crucial to solving many real-world problems.
        </p>
        <ul>
          <li>
            <img src="svg/monitor.svg" alt="work" width="16" height="16"> &nbsp;&nbsp;
            Senior Research Engineer, <a href="https://www.deepmind.com/">Google DeepMind</a>, London
          </li>
          <li>
            <img src="svg/education.svg" alt="education" width="16" height="16"> &nbsp;&nbsp;
            PhD, <a href="https://www.ucl.ac.uk/">University College London</a>, Thesis: Multiagent Training in N-Player General-Sum Games
          </li>
          <li>
            <img src="svg/education.svg" alt="education" width="16" height="16"> &nbsp;&nbsp;
            Information Engineering, Bachelors and Masters, First Class, <a href="https://www.ucl.ac.uk/">University of Cambridge</a>
          </li>
        </ul>
      </section>
      
      <section class="container  py-2">
        <h2>Software and Libraries</h2>
        <ul class="big-bullet">
          <li class="mb-4">
            <h4>twoxtwogame LaTeX Package</h4>
            <p>
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://arxiv.org/abs/2402.16985">arXiv 2024</a>, <a href="https://github.com/google-deepmind/twoxtwogame">GitHub</a>, <a href="https://ctan.org/pkg/twoxtwogame">CTAN</a>
            </p>
            <p class="lh-sm">
              This is a package for the visualization of 2x2 normal form games. The package is based on PGF/TikZ and produces beautiful vector graphics that are indented for use in scientific publications. The commands include the creation of graphical representations of 2x2 games, the visualization of equilibria in 2x2 games and game embeddings for 2x2 games.
            </p>
          </li>
        </ul>
        
        <h2>Papers and Publications</h2>
        <ul class="big-bullet">
          <li class="mb-4">
            <h4>Visualizing 2x2 Normal-Form Games: twoxtwogame LaTeX Package</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2024">2024</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              <strong>Luke Marris</strong>, Ian Gemp, Siqi Liu, Joel Z. Leibo, Georgios Piliouras
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://arxiv.org/abs/2402.16985">arXiv 2024</a>, <a href="https://github.com/google-deepmind/twoxtwogame">GitHub</a>, <a href="https://ctan.org/pkg/twoxtwogame">CTAN</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Normal-Form Games, 2x2 Games, Latex, Visualization
            </p>
            <p class="lh-sm">
              Normal-form games with two players, each with two strategies, are the most studied class of games. These so-called 2x2 games are used to model a variety of strategic interactions. They appear in game theory, economics, and artificial intelligence research. However, there lacks tools for describing and visualizing such games. This work introduces a LaTeX package for visualizing 2x2 games. This work has two goals: first, to provide high-quality tools and vector graphic visualizations, suitable for scientific publications. And second, to help promote standardization of names and representations of 2x2 games. The LaTeX package, twoxtwogame, is maintained on GitHub and mirrored on CTAN, and is available under a permissive Apache 2 license.
            </p>
          </li>
          <li class="mb-4">
            <h4>NfgTransformer: Equivariant Representation Learning for Normal-form Games</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2024">2024</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              Siqi Liu, <strong>Luke Marris</strong>, Georgios Piliouras, Ian Gemp, Nicolas Heess
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://arxiv.org/abs/2402.08393">arXiv 2024</a>, <a href="https://openreview.net/forum?id=4YESQqIys7">Open Review 2024</a>, <a href="https://github.com/google-deepmind/nfg_transformer">GitHub 2024</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Normal-Form Games, Neural Networks, Game Thoery, Representations
            </p>
            <p class="lh-sm">
              Normal-form games (NFGs) are the fundamental model of strategic interaction. We study their representation using neural networks. We describe the inherent equivariance of NFGs -- any permutation of strategies describes an equivalent game -- as well as the challenges this poses for representation learning. We then propose the NfgTransformer architecture that leverages this equivariance, leading to state-of-the-art performance in a range of game-theoretic tasks including equilibrium-solving, deviation gain estimation and ranking, with a common approach to NFG representation. We show that the resulting model is interpretable and versatile, paving the way towards deep learning systems capable of game-theoretic reasoning when interacting with humans and with each other.
            </p>
          </li>
          <li class="mb-4">
            <h4>Approximating the Core via Iterative Coalition Sampling</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2024">2024</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              Ian Gemp, Marc Lanctot, <strong>Luke Marris</strong>, Yiran Mao, Edgar Duéñez-Guzmán, Sarah Perrin, Andras Gyorgy, Romuald Elie, Georgios Piliouras, Michael Kaisers, Daniel Hennes, Kalesha Bullard, Kate Larson, Yoram Bachrach
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://arxiv.org/abs/2402.03928">arXiv 2024</a>,
              <a href="https://www.aamas2024-conference.auckland.ac.nz/accepted/papers/">AAMAS 2024</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Core, Neural Networks, Game Thoery
            </p>
            <p class="lh-sm">
              The core is a central solution concept in cooperative game theory, defined as the set of feasible allocations or payments such that no subset of agents has incentive to break away and form their own subgroup or coalition. However, it has long been known that the core (and approximations, such as the least-core) are hard to compute. This limits our ability to analyze cooperative games in general, and to fully embrace cooperative game theory contributions in domains such as explainable AI (XAI), where the core can complement the Shapley values to identify influential features or instances supporting predictions by black-box models. We propose novel iterative algorithms for computing variants of the core, which avoid the computational bottleneck of many other approaches; namely solving large linear programs. As such, they scale better to very large problems as we demonstrate across different classes of cooperative games, including weighted voting games, induced subgraph games, and marginal contribution networks. We also explore our algorithms in the context of XAI, providing further evidence of the power of the core for such applications.
            </p>
          </li>
          <li class="mb-4">
            <h4>States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2024">2024</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              Ian Gemp, Yoram Bachrach, Marc Lanctot, Roma Patel, Vibhavari Dasagi, <strong>Luke Marris</strong>, Georgios Piliouras, Karl Tuyls
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://arxiv.org/abs/2402.01704">arXiv 2024</a>,
              <a href="https://github.com/google-deepmind/open_spiel/tree/master/open_spiel/python/games/chat_games">GitHub 2024</a>,
              <a href="https://www.wired.com/story/game-theory-can-make-ai-more-correct-and-efficient/">Wired 2024</a>,
              <a href="https://www.quantamagazine.org/game-theory-can-make-ai-more-correct-and-efficient-20240509/">Quanta 2024</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Large Language Models, Game Theory
            </p>
            <p class="lh-sm">
              Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new dialogue scenarios, which are grounded in real world applications. In this work, we present one possible binding from dialogue to game theory as well as generalizations of existing equilibrium finding algorithms to this setting. In addition, by exploiting LLMs generation capabilities along with our proposed binding, we can synthesize a large repository of formally-defined games in which one can study and test game-theoretic solution concepts. We also demonstrate how one can combine LLM-driven game generation, game-theoretic solvers, and imitation learning to construct a process for improving the strategic capabilities of LLMs.
            </p>
          </li>
          <li class="mb-4">
            <h4>Neural Population Learning beyond Symmetric Zero-sum Games</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2024">2024</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              Siqi Liu, <strong>Luke Marris</strong>, Marc Lanctot, Georgios Piliouras, Joel Z Leibo, Nicolas Heess
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://arxiv.org/abs/2401.05133">arXiv 2024</a>,
              <a href="https://www.aamas2024-conference.auckland.ac.nz/accepted/papers/">AAMAS 2024</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              NeuPL, CCE, n-player, general-sum, JPSRO
            </p>
            <p class="lh-sm">
              We study computationally efficient methods for finding equilibria in n-player general-sum games, specifically ones that afford complex visuomotor skills. We show how existing methods would struggle in this setting, either computationally or in theory. We then introduce NeuPL-JPSRO, a neural population learning algorithm that benefits from transfer learning of skills and converges to a Coarse Correlated Equilibrium (CCE) of the game. We show empirical convergence in a suite of OpenSpiel games, validated rigorously by exact game solvers. We then deploy NeuPL-JPSRO to complex domains, where our approach enables adaptive coordination in a MuJoCo control domain and skill transfer in capture-the-flag. Our work shows that equilibrium convergent population learning can be implemented at scale and in generality, paving the way towards solving real-world games between heterogeneous players with mixed motives.
            </p>
          </li>
          <li class="mb-4">
            <h4>Evaluating Agents using Social Choice Theory</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2023">2023</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              Marc Lanctot, Kate Larson, Yoram Bachrach, <strong>Luke Marris</strong>, Zun Li, Avishkar Bhoopchand, Thomas Anthony, Brian Tanner, Anna Koop
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://arxiv.org/abs/2312.03121">arXiv 2023</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Ranking, Rating, Social Choice Theory
            </p>
            <p class="lh-sm">
              We argue that many general evaluation problems can be viewed through the lens of voting theory. Each task is interpreted as a separate voter, which requires only ordinal rankings or pairwise comparisons of agents to produce an overall evaluation. By viewing the aggregator as a social welfare function, we are able to leverage centuries of research in social choice theory to derive principled evaluation frameworks with axiomatic foundations. These evaluations are interpretable and flexible, while avoiding many of the problems currently facing cross-task evaluation. We apply this Voting-as-Evaluation (VasE) framework across multiple settings, including reinforcement learning, large language models, and humans. In practice, we observe that VasE can be more robust than popular evaluation frameworks (Elo and Nash averaging), discovers properties in the evaluation data not evident from scores alone, and can predict outcomes better than Elo in a complex seven-player game. We identify one particular approach, maximal lotteries, that satisfies important consistency properties relevant to evaluation, is computationally efficient (polynomial in the size of the evaluation data), and identifies game-theoretic cycles.
            </p>
          </li>
          <li class="mb-4">
            <h4>Generative Adversarial Equilibrium Solvers</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2023">2023</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              Denizalp Goktas, David C Parkes, Ian Gemp, <strong>Luke Marris</strong>, Georgios Piliouras, Romuald Elie, Guy Lever, Andrea Tacchetti
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://arxiv.org/abs/2302.06607">arXiv 2023</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Generalized Nash Equilibrium, Neural Networks
            </p>
            <p class="lh-sm">
              We introduce the use of generative adversarial learning to compute equilibria in general game-theoretic settings, specifically the generalized Nash equilibrium (GNE) in pseudo-games, and its specific instantiation as the competitive equilibrium (CE) in Arrow-Debreu competitive economies. Pseudo-games are a generalization of games in which players' actions affect not only the payoffs of other players but also their feasible action spaces. Although the computation of GNE and CE is intractable in the worst-case, i.e., PPAD-hard, in practice, many applications only require solutions with high accuracy in expectation over a distribution of problem instances. We introduce Generative Adversarial Equilibrium Solvers (GAES): a family of generative adversarial neural networks that can learn GNE and CE from only a sample of problem instances. We provide computational and sample complexity bounds, and apply the framework to finding Nash equilibria in normal-form games, CE in Arrow-Debreu competitive economies, and GNE in an environmental economic model of the Kyoto mechanism.
            </p>
          </li>
          <li class="mb-4">
            <h4>Approximating Nash Equilibria in Normal-Form Games via Stochastic Optimization</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2023">2023</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              Ian Gemp, <strong>Luke Marris</strong>, Georgios Piliouras
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://arxiv.org/abs/2310.06689">arXiv 2023</a>,
              <a href="https://openreview.net/forum?id=cc8h3I3V4E">ICLR OpenReview 2024</a>,
              <a href="https://blog.iclr.cc/2024/05/06/iclr-2024-outstanding-paper-awards/">ICLR Honorable Mention 2024</a>,
              <a href="https://iclr.cc/virtual/2024/oral/19744">ICLR Oral 2024</a>,
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Nash Equilibrium, Polymatrix Game, Normal-Form Game
            </p>
            <p class="lh-sm">
              We propose the first, to our knowledge, loss function for approximate Nash equilibria of normal-form games that is amenable to unbiased Monte Carlo estimation. This construction allows us to deploy standard non-convex stochastic optimization techniques for approximating Nash equilibria, resulting in novel algorithms with provable guarantees. We complement our theoretical analysis with experiments demonstrating that stochastic gradient descent can outperform previous state-of-the-art approaches.
            </p>
          </li>
          <li class="mb-4">
            <h4>Equilibrium-Invariant Embedding, Metric Space, and Fundamental Set of 2×2 Normal-Form Games</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2023">2023</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              <strong>Luke Marris</strong>, Ian Gemp, Georgios Piliouras
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://arxiv.org/abs/2304.09978">arXiv 2023</a>,
              <a href="https://www.deepmind.com/publications/equilibrium-invariant-embedding-metric-space-and-fundamental-set-of-22-normal-form-games">DeepMind</a>,
              <a href="./assets/embeddings_ecatec2023.pdf">EC&#64;EC 2023 Poster</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Embedding, Invariance, Nash Equilibrium, Correlated Equilibrium, Dimensionality Reduction, Visualization
            </p>
            <p class="lh-sm">
              Equilibrium solution concepts of normal-form games, such as Nash equilibria, correlated equilibria, and coarse correlated equilibria, describe the joint strategy profiles from which no player has incentive to unilaterally deviate. They are widely studied in game theory, economics, and multiagent systems. Equilibrium concepts are invariant under certain transforms of the payoffs. We define an equilibrium-inspired distance metric for the space of all normal-form games and uncover a distance-preserving equilibrium-invariant embedding. Furthermore, we propose an additional transform which defines a better-response-invariant distance metric and embedding. To demonstrate these metric spaces we study 2×2 games. The equilibrium-invariant embedding of 2×2 games has an efficient two variable parameterization (a reduction from eight), where each variable geometrically describes an angle on a unit circle. Interesting properties can be spatially inferred from the embedding, including: equilibrium support, cycles, competition, coordination, distances, best-responses, and symmetries. The best-response-invariant embedding of 2×2 games, after considering symmetries, rediscovers a set of 15 games, and their respective equivalence classes. We propose that this set of game classes is fundamental and captures all possible interesting strategic interactions in 2×2 games. We introduce a directed graph representation and name for each class. Finally, we leverage the tools developed for 2×2 games to develop game theoretic visualizations of large normal-form and extensive-form games that aim to fingerprint the strategic interactions that occur within.
            </p>
          </li>
          <li class="mb-4">
            <h4>Search-Improved Game-Theoretic Multiagent Reinforcement Learning in General and Negotiation Games</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2023">2023</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              Zun Li, Marc Lanctot, Kevin McKee, <strong>Luke Marris</strong>, Ian Gemp, Daniel Hennes, Paul Muller, Kate Larson, Yoram Bachrach, Michael P. Wellman
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://arxiv.org/abs/2302.00797">arXiv 2023</a>,
              <a href="https://aamas2023.soton.ac.uk/program/accepted-papers/">AAMAS 2023</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Algorithms, Nash Bargaining, Correlated Equilibrium
            </p>
            <p class="lh-sm">
              Multiagent reinforcement learning (MARL) has benefited significantly from population-based and game-theoretic training regimes. One approach, Policy-Space Response Oracles (PSRO), employs standard reinforcement learning to compute response policies via approximate best responses and combines them via meta-strategy selection. We augment PSRO by adding a novel search procedure with generative sampling of world states, and introduce two new meta-strategy solvers based on the Nash bargaining solution. We evaluate PSRO's ability to compute approximate Nash equilibrium, and its performance in two negotiation games: Colored Trails, and Deal or No Deal. We conduct behavioral studies where human participants negotiate with our agents (N=346). We find that search with generative modeling finds stronger policies during both training time and test time, enables online Bayesian co-player prediction, and can produce agents that achieve comparable social welfare negotiating with humans as humans trading among themselves.
            </p>
          </li>
          <li class="mb-4">
            <h4>Turbocharging Solution Concepts: Solving NEs, CEs and CCEs with Neural Equilibrium Solvers</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2022">2022</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              <strong>Luke Marris</strong>, Ian Gemp, Thomas Anthony, Andrea Tacchetti, Siqi Liu, Karl Tuyls
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://arxiv.org/abs/2210.09257">arXiv 2022</a>,
              <a href="https://papers.nips.cc/paper_files/paper/2022/hash/24f420aa4c99642dbb9aae18b166bbbc-Abstract-Conference.html">NeurIPS 2022</a>,
              <a href="https://nips.cc/virtual/2022/poster/53003">Poster and Presentation</a>,
              <a href="https://openreview.net/forum?id=RczPtvlaXPH">Open Review</a>,
              <a href="./assets/nes_ecatec2023.pdf">EC&#64;EC 2023 Poster</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Deep Neural Networks, Invariant Architecture, Equivariant Architecture, Dual Optimization, Invariant Embedding, Corellated Equilibrim, Coarse Corellated Equilibrium, Semi-Supervised Learning
            </p>
            <p class="lh-sm">
              Solution concepts such as Nash Equilibria, Correlated Equilibria, and Coarse Correlated Equilibria are useful components for many multiagent machine learning algorithms. Unfortunately, solving a normal-form game could take prohibitive or non-deterministic time to converge, and could fail. We introduce the Neural Equilibrium Solver which utilizes a special equivariant neural network architecture to approximately solve the space of all games of fixed shape, buying speed and determinism. We define a flexible equilibrium selection framework, that is capable of uniquely selecting an equilibrium that minimizes relative entropy, or maximizes welfare. The network is trained without needing to generate any supervised training data. We show remarkable zero-shot generalization to larger games. We argue that such a network is a powerful component for many possible multiagent algorithms.
            </p>
          </li>
          <li class="mb-4">
            <h4>Game Theoretic Rating in N-player general-sum games with Equilibria</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2022">2022</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              <strong>Luke Marris</strong>, Marc Lanctot, Ian Gemp, Shayegan Omidshafiei, Stephen McAleer, Jerome Connor, Karl Tuyls, Thore Graepel
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://arxiv.org/abs/2210.02205">arXiv 2022</a>,
              <a href="https://www.deepmind.com/publications/game-theoretic-rating-in-n-player-general-sum-games-with-equilibria">DeepMind</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Rating, Ranking, Nash Equilibrium, Correlated Equilibrium, Coarse Correlated Equilibrium, Nash Average
            </p>
            <p class="lh-sm">
              Rating strategies in a game is an important area of research in game theory and artificial intelligence, and can be applied to any real-world competitive or cooperative setting. Traditionally, only transitive dependencies between strategies have been used to rate strategies (e.g. Elo), however recent work has expanded ratings to utilize game theoretic solutions to better rate strategies in non-transitive games. This work generalizes these ideas and proposes novel algorithms suitable for N-player, general-sum rating of strategies in normal-form games according to the payoff rating system. This enables well-established solution concepts, such as equilibria, to be leveraged to efficiently rate strategies in games with complex strategic interactions, which arise in multiagent training and real-world interactions between many agents. We empirically validate our methods on real world normal-form data (Premier League) and multiagent reinforcement learning agent evaluation.
            </p>
          </li>
          <li class="mb-4">
            <h4>Developing, evaluating and scaling learning agents in multi-agent environments</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2022">2022</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              Ian Gemp, Thomas Anthony, Yoram Bachrach, Avishkar Bhoopchand, Kalesha Bullard, Jerome Connor, Vibhavari Dasagi, Bart De Vylder, Edgar A Duéñez-Guzmán, Romuald Elie, Richard Everett, Daniel Hennes, Edward Hughes, Mina Khan, Marc Lanctot, Kate Larson, Guy Lever, Siqi Liu, <strong>Luke Marris</strong>, Kevin R McKee, Paul Muller, Julien Pérolat, Florian Strub, Andrea Tacchetti, Eugene Tarassov, Zhe Wang, Karl Tuyls
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://content.iospress.com/articles/ai-communications/aic220113">AI Communications</a>,
              <a href="https://arxiv.org/abs/2209.10958">arXiv 2022</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Review
            </p>
            <p class="lh-sm">
              The Game Theory & Multi-Agent team at DeepMind studies several aspects of multi-agent learning ranging from computing approximations to fundamental concepts in game theory to simulating social dilemmas in rich spatial environments and training 3-d humanoids in difficult team coordination tasks. A signature aim of our group is to use the resources and expertise made available to us at DeepMind in deep reinforcement learning to explore multi-agent systems in complex environments and use these benchmarks to advance our understanding. Here, we summarise the recent work of our team and present a taxonomy that we feel highlights many important open challenges in multi-agent research.
            </p>
          </li>
          <li class="mb-4">
            <h4>Simplex Neural Population Learning: Any-Mixture Bayes-Optimality in Symmetric Zero-sum Games</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2022">2022</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              S Liu, M Lanctot, <strong>L Marris</strong>, N Heess
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://proceedings.mlr.press/v162/liu22h.html">ICML 2022</a>,
              <a href="https://arxiv.org/abs/2205.15879">arXiv 2022</a>,
              <a href="https://icml.cc/virtual/2022/poster/18415">Poster</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Algroithms, Equilibrium Computation, Conditional Policy, Generalization 
            </p>
            <p class="lh-sm">
              Learning to play optimally against any mixture over a diverse set of strategies is of important practical interests in competitive games. In this paper, we propose simplex-NeuPL that satisfies two desiderata simultaneously: i) learning a population of strategically diverse basis policies, represented by a single conditional network; ii) using the same network, learn best-responses to any mixture over the simplex of basis policies. We show that the resulting conditional policies incorporate prior information about their opponents effectively, enabling near optimal returns against arbitrary mixture policies in a game with tractable best-responses. We verify that such policies behave Bayes-optimally under uncertainty and offer insights in using this flexibility at test time. Finally, we offer evidence that learning best-responses to any mixture policies is an effective auxiliary task for strategic exploration, which, by itself, can lead to more performant populations.
            </p>
          </li>
          <li class="mb-4">
            <h4>NeuPL: Neural Population Learning</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2022">2022</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              S Liu, <strong>L Marris</strong>, D Hennes, J Merel, N Heess, T Graepel
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://openreview.net/forum?id=MIX3fJkl_1">ICLR 2022</a>,
              <a href="https://arxiv.org/abs/2202.07415">arXiv 2022</a>,
              <a href="https://iclr.cc/media/iclr-2022/Slides/6950.pdf">Slides</a>,
              <a href="https://neupl.github.io/demo/">Website</a>,
              <a href="https://iclr.cc/virtual/2022/poster/6950">Talk</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Algroithms, Equilibrium Computation, Conditional Policy, Generalization 
            </p>
            <p class="lh-sm">
              Learning in strategy games (e.g. StarCraft, poker) requires the discovery of diverse policies. This is often achieved by iteratively training new policies against existing ones, growing a policy population that is robust to exploit. This iterative approach suffers from two issues in real-world games: a) under finite budget, approximate best-response operators at each iteration needs truncating, resulting in under-trained good-responses populating the population; b) repeated learning of basic skills at each iteration is wasteful and becomes intractable in the presence of increasingly strong opponents. In this work, we propose Neural Population Learning (NeuPL) as a solution to both issues. NeuPL offers convergence guarantees to a population of best-responses under mild assumptions. By representing a population of policies within a single conditional model, NeuPL enables transfer learning across policies. Empirically, we show the generality, improved performance and efficiency of NeuPL across several test domains. Most interestingly, we show that novel strategies become more accessible, not less, as the neural population expands.
            </p>
           </li>
          <li class="mb-4">
            <h4>Multi-agent training beyond zero-sum with correlated equilibrium meta-solvers</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2021">2021</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              <strong>L Marris</strong>, P Muller, M Lanctot, K Tuyls, T Graepel
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://proceedings.mlr.press/v139/marris21a">ICML 2021</a>,
              <a href="https://arxiv.org/abs/2106.09435">arXiv 2021</a>,
              <a href="https://icml.cc/media/icml-2021/Slides/10003.pdf">Slides</a>,
              <a href="https://papertalk.org/papertalks/32703">Talk</a>,
              <a href="https://paperswithcode.com/paper/multi-agent-training-beyond-zero-sum-with">Code</a>,
              <a href="https://www.deepmind.com/publications/multi-agent-training-beyond-zero-sum-with-correlated-equilibrium-meta-solvers">DeepMind</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Algroithms, Equilibrium Computation, JPSRO, MGCE, Correlated Equilibrium, Coarse Correlated Equilibrium, PSRO, Self Play
            </p>
            <p class="lh-sm">
              Two-player, constant-sum games are well studied in the literature, but there has been limited progress outside of this setting. We propose Joint Policy-Space Response Oracles (JPSRO), an algorithm for training agents in n-player, general-sum extensive form games, which provably converges to an equilibrium. We further suggest correlated equilibria (CE) as promising meta-solvers, and propose a novel solution concept Maximum Gini Correlated Equilibrium (MGCE), a principled and computationally efficient family of solutions for solving the correlated equilibrium selection problem. We conduct several experiments using CE meta-solvers for JPSRO and demonstrate convergence on n-player, general-sum games.
            </p>
          </li>
          <li class="mb-4">
            <h4>From motor control to team play in simulated humanoid football</h4>            
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2021">2021</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              Siqi Liu, Guy Lever, Zhe Wang, Josh Merel, SM Eslami, Daniel Hennes, Wojciech M Czarnecki, Yuval Tassa, Shayegan Omidshafiei, Abbas Abdolmaleki, Noah Y Siegel, Leonard Hasenclever, <strong>Luke Marris</strong>, Saran Tunyasuvunakool, H Francis Song, Markus Wulfmeier, Paul Muller, Tuomas Haarnoja, Brendan D Tracey, Karl Tuyls, Thore Graepel, Nicolas Heess
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://www.science.org/doi/10.1126/scirobotics.abo0235">Science Robotics</a>,
              <a href="https://arxiv.org/abs/2105.12196">arXiv 2021</a> <a href="https://youtu.be/KHMwq9pv7mg">Video</a>,
              <a href="https://www.youtube.com/watch?v=HTON7odbW0o">Two Minute Papers</a>,
              <a href="https://youtu.be/880TBXMuzmk?t=980">60 Minutes</a>,
              <a href="https://www.weforum.org/agenda/2022/09/robot-football-advance-artificial-intelligence-and-automation/">World Economic Forum</a>,
              <a href="https://www.wired.com/story/alphabet-deepmind-ai-humanoids-soccer-camp/">WIRED</a>,
              <a href="https://www.newscientist.com/article/2336132-deepmind-ai-learns-to-play-soccer-using-decades-of-match-simulations/">New Scientist</a>,
              <a href="https://www.deepmind.com/blog/from-motor-control-to-embodied-intelligence">DeepMind Blog</a>,
              <a href="https://robotics.altmetric.com/details/135225846">Altmetrics</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Multi-Agent, Physics, Football, Soccer
            </p>
            <p class="lh-sm">
              Intelligent behaviour in the physical world exhibits structure at multiple spatial and temporal scales. Although movements are ultimately executed at the level of instantaneous muscle tensions or joint torques, they must be selected to serve goals defined on much longer timescales, and in terms of relations that extend far beyond the body itself, ultimately involving coordination with other agents. Recent research in artificial intelligence has shown the promise of learning-based approaches to the respective problems of complex movement, longer-term planning and multi-agent coordination. However, there is limited research aimed at their integration. We study this problem by training teams of physically simulated humanoid avatars to play football in a realistic virtual environment. We develop a method that combines imitation learning, single- and multi-agent reinforcement learning and population-based training, and makes use of transferable representations of behaviour for decision making at different levels of abstraction. In a sequence of stages, players first learn to control a fully articulated body to perform realistic, human-like movements such as running and turning; they then acquire mid-level football skills such as dribbling and shooting; finally, they develop awareness of others and play as a team, bridging the gap between low-level motor control at a timescale of milliseconds, and coordinated goal-directed behaviour as a team at the timescale of tens of seconds. We investigate the emergence of behaviours at different levels of abstraction, as well as the representations that underlie these behaviours using several analysis techniques, including statistics from real-world sports analytics. Our work constitutes a complete demonstration of integrated decision-making at multiple scales in a physically embodied multi-agent setting.
            </p>
          </li>
          <li class="mb-4">
            <h4>Backpropagation and the brain</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2020">2020</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              Timothy P Lillicrap, Adam Santoro, <strong>Luke Marris</strong>, Colin J Akerman, Geoffrey Hinton
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://www.nature.com/articles/s41583-020-0277-3">Nature Reviews Neuroscience</a>,
              <a href="https://ora.ox.ac.uk/objects/uuid:862189c1-0088-4f78-b17a-2748c2019209/download_file?safe_filename=Lillicrap_v6_2020.pdf&type_of_work=Journal+article">Open Access</a>,
              <a href="https://singularityhub.com/2020/05/05/biological-to-artificial-and-back-a-core-ai-algorithm-may-work-in-the-brain/">Singularity Hub</a>,
              <a href="https://nature.altmetric.com/details/80014251">Altmetric</a>
              &nbsp;
              <img src="svg/key.svg" alt="keywords" width="16" height="16">
              Biologically Plausible
            </p>
            <p class="lh-sm">
              During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.
            </p>
          </li>
          <li class="mb-4">
            <h4>A generalized training approach for multiagent learning</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2019">2019</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              Paul Muller, Shayegan Omidshafiei, Mark Rowland, Karl Tuyls, Julien Perolat, Siqi Liu, Daniel Hennes, <strong>Luke Marris</strong>, Marc Lanctot, Edward Hughes, Zhe Wang, Guy Lever, Nicolas Heess, Thore Graepel, Remi Munos
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://iclr.cc/virtual_2020/poster_Bkl5kxrKDr.html">ICLR 2019</a>,
              <a href="https://arxiv.org/abs/1909.12823">arXiv 2019</a>
            </p>
            <p class="lh-sm">
              This paper investigates a population-based training regime based on game-theoretic principles called Policy-Spaced Response Oracles (PSRO). PSRO is general in the sense that it (1) encompasses well-known algorithms such as fictitious play and double oracle as special cases, and (2) in principle applies to general-sum, many-player games. Despite this, prior studies of PSRO have been focused on two-player zero-sum games, a regime wherein Nash equilibria are tractably computable. In moving from two-player zero-sum games to more general settings, computation of Nash equilibria quickly becomes infeasible. Here, we extend the theoretical underpinnings of PSRO by considering an alternative solution concept, α-Rank, which is unique (thus faces no equilibrium selection issues, unlike Nash) and applies readily to general-sum, many-player settings. We establish convergence guarantees in several games classes, and identify links between Nash equilibria and α-Rank. We demonstrate the competitive performance of α-Rank-based PSRO against an exact Nash solver-based PSRO in 2-player Kuhn and Leduc Poker. We then go beyond the reach of prior PSRO applications by considering 3- to 5-player poker games, yielding instances where α-Rank achieves faster convergence than approximate Nash solvers, thus establishing it as a favorable general games solver. We also carry out an initial empirical validation in MuJoCo soccer, illustrating the feasibility of the proposed approach in another complex domain.
            </p>
          </li>
          <li class="mb-4">
            <h4>Human-level performance in 3D multiplayer games with population-based reinforcement learning</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2019">2019</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, <strong>Luke Marris</strong>, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, Thore Graepel
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://www.science.org/doi/10.1126/science.aau6249">Science</a>,
              <a href="https://arxiv.org/abs/1807.01281">arXiv 2019</a>,
              <a href="https://www.deepmind.com/blog/capture-the-flag-the-emergence-of-complex-cooperative-agents">DeepMind Blog</a>,
              <a href="https://www.youtube.com/watch?v=MvFABFWPBrw">Two Minute Papers</a>,
              <a href="https://arstechnica.com/science/2019/05/googles-ai-group-moves-on-from-go-tackles-quake-iii-arena/">Ars Technica</a>,
              <a href="https://www.forbes.com/sites/maxthielmeyer/2019/06/01/googles-deepmind-ai-learns-to-beat-human-players-at-quake-iii/">Forbes</a>,
              <a href="https://www.nytimes.com/2019/05/30/science/deep-mind-artificial-intelligence.html">New York Times</a>,
              <a href="https://www.newscientist.com/article/2204978-deepminds-ai-gamer-is-a-better-teammate-than-human-players/">New Scientist</a>,
              <a href="https://science.altmetric.com/details/44459807">Altmetric</a>
            </p>
            <p class="lh-sm">
              Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.
            </p>
          </li>
          <li class="mb-4">
            <h4>Assessing the scalability of biologically-motivated deep learning algorithms and architectures</h4>
            <p>
              <img src="svg/calendar.svg" alt="publication date" width="16" height="16"> <time datetime="2018">2018</time>
              &nbsp;
              <img src="svg/authors.svg" alt="authors" width="16" height="16">
              S Bartunov, A Santoro, B Richards, <strong>L Marris</strong>, GE Hinton, T Lillicrap
              &nbsp;
              <img src="svg/link.svg" alt="links" width="16" height="16">
              <a href="https://papers.nips.cc/paper/2018/hash/63c3ddcc7b23daa1e42dc41f9a44a873-Abstract.html">NeurIPS 2018</a>,
              <a href="https://arxiv.org/abs/1807.04587">arXiv 2018</a>
            </p>
            <p class="lh-sm">
              The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.
            </p>
          </li>
        </ul>
      </section>
    </main>

    <footer class="footer mt-auto py-3 bg-light">
      <div class="container text-muted text-center">
        <address><img src="svg/home.svg" alt="home"> London</address>
        <p>
          Follow me on <a href="https://twitter.com/MarrisLuke">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-twitter" viewBox="0 0 16 16">
              <path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334 0-.14 0-.282-.006-.422A6.685 6.685 0 0 0 16 3.542a6.658 6.658 0 0 1-1.889.518 3.301 3.301 0 0 0 1.447-1.817 6.533 6.533 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.325 9.325 0 0 1-6.767-3.429 3.289 3.289 0 0 0 1.018 4.382A3.323 3.323 0 0 1 .64 6.575v.045a3.288 3.288 0 0 0 2.632 3.218 3.203 3.203 0 0 1-.865.115 3.23 3.23 0 0 1-.614-.057 3.283 3.283 0 0 0 3.067 2.277A6.588 6.588 0 0 1 .78 13.58a6.32 6.32 0 0 1-.78-.045A9.344 9.344 0 0 0 5.026 15z"/>
            </svg> twitter</a>
          and <a href="https://www.threads.net/@marrisluke">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-threads" viewBox="0 0 16 16">
              <path d="M6.321 6.016c-.27-.18-1.166-.802-1.166-.802.756-1.081 1.753-1.502 3.132-1.502.975 0 1.803.327 2.394.948.591.621.928 1.509 1.005 2.644.328.138.63.299.905.484 1.109.745 1.719 1.86 1.719 3.137 0 2.716-2.226 5.075-6.256 5.075C4.594 16 1 13.987 1 7.994 1 2.034 4.482 0 8.044 0 9.69 0 13.55.243 15 5.036l-1.36.353C12.516 1.974 10.163 1.43 8.006 1.43c-3.565 0-5.582 2.171-5.582 6.79 0 4.143 2.254 6.343 5.63 6.343 2.777 0 4.847-1.443 4.847-3.556 0-1.438-1.208-2.127-1.27-2.127-.236 1.234-.868 3.31-3.644 3.31-1.618 0-3.013-1.118-3.013-2.582 0-2.09 1.984-2.847 3.55-2.847.586 0 1.294.04 1.663.114 0-.637-.54-1.728-1.9-1.728-1.25 0-1.566.405-1.967.868ZM8.716 8.19c-2.04 0-2.304.87-2.304 1.416 0 .878 1.043 1.168 1.6 1.168 1.02 0 2.067-.282 2.232-2.423a6.217 6.217 0 0 0-1.528-.161Z"/>
            </svg> Threads</a>.
<!--             Join me on <a href="https://mastodonapp.uk/@marris">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-mastodon" viewBox="0 0 16 16">
              <path d="M11.19 12.195c2.016-.24 3.77-1.475 3.99-2.603.348-1.778.32-4.339.32-4.339 0-3.47-2.286-4.488-2.286-4.488C12.062.238 10.083.017 8.027 0h-.05C5.92.017 3.942.238 2.79.765c0 0-2.285 1.017-2.285 4.488l-.002.662c-.004.64-.007 1.35.011 2.091.083 3.394.626 6.74 3.78 7.57 1.454.383 2.703.463 3.709.408 1.823-.1 2.847-.647 2.847-.647l-.06-1.317s-1.303.41-2.767.36c-1.45-.05-2.98-.156-3.215-1.928a3.614 3.614 0 0 1-.033-.496s1.424.346 3.228.428c1.103.05 2.137-.064 3.188-.189zm1.613-2.47H11.13v-4.08c0-.859-.364-1.295-1.091-1.295-.804 0-1.207.517-1.207 1.541v2.233H7.168V5.89c0-1.024-.403-1.541-1.207-1.541-.727 0-1.091.436-1.091 1.296v4.079H3.197V5.522c0-.859.22-1.541.66-2.046.456-.505 1.052-.764 1.793-.764.856 0 1.504.328 1.933.983L8 4.39l.417-.695c.429-.655 1.077-.983 1.934-.983.74 0 1.336.259 1.791.764.442.505.661 1.187.661 2.046v4.203z"/>
            </svg> Mastodon</a>. -->
          Connect on <a href="https://www.linkedin.com/in/lukemarris/">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-linkedin" viewBox="0 0 16 16">
              <path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z"/>
            </svg> LinkedIn</a>.
          View my <a href="https://scholar.google.com/citations?user=dvTeSX4AAAAJ">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-mortarboard-fill" viewBox="0 0 16 16">
            <path d="M8.211 2.047a.5.5 0 0 0-.422 0l-7.5 3.5a.5.5 0 0 0 .025.917l7.5 3a.5.5 0 0 0 .372 0L14 7.14V13a1 1 0 0 0-1 1v2h3v-2a1 1 0 0 0-1-1V6.739l.686-.275a.5.5 0 0 0 .025-.917l-7.5-3.5Z"/>
            <path d="M4.176 9.032a.5.5 0 0 0-.656.327l-.5 1.7a.5.5 0 0 0 .294.605l4.5 1.8a.5.5 0 0 0 .372 0l4.5-1.8a.5.5 0 0 0 .294-.605l-.5-1.7a.5.5 0 0 0-.656-.327L8 10.466 4.176 9.032Z"/>
          </svg> Google Scholar</a>.
        </p>
        <p>Made with ♣♦♥♠</p>
      </div>
    </footer>

    <!-- JavaScript Bundle with Popper -->
    <script
        src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-OERcA2EqjJCMA+/3y+gxIOqMEjwtxJY7qPCqsdltbNJuaOe923+mo//f6V8Qbsw3"
        crossorigin="anonymous">
    </script>
      
  </body>
</html>
