<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Luke Marris' personal wesbite.">
    <meta name="keywords" content="Luke Marris, N-player, General-Sum, Solution Concepts, Equilibrium Selection, Nash Equilibrium, NE, Correlated Equilibrium, CE, Coarse Correlated Equilibrium, CCE, DeepMind, Cambridge, UCL, Jesus College, Reinforcement Learning, RL, Multiagent, MARL, PSRO, JPSRO, MGCE, Payoff Rating, Elo, Nash Average, Game Theory, Multiagent Systems, ICML, NeurIPS, ICLR">
    <meta name="author" content="Luke Marris">
    <title>Luke Marris</title>

    <!-- Bootstrap core CSS -->
    <link
        href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css"
        rel="stylesheet"
        integrity="sha384-Zenh87qX5JnK2Jl0vWa8Ck2rdkQ2Bzep5IDxbcnCeuOxjzrPF/et3URy9Bv1WTRi"
        crossorigin="anonymous">

    <!-- Favicons -->
    <link rel="icon" href="favicon.ico">
    
    <!-- Privacy Friendly Analytics -->
    <script src="https://cdn.counter.dev/script.js" data-id="08224b8c-5ef2-4b5a-a31e-342a17c8f243" data-utcoffset="1"></script>

    <style>
      @media (min-width: 1200px) {
          .container{
              max-width: 970px;
          }
      }
      
      .white-shadow {
          text-shadow: #000 0px 0px 2px,   #000 0px 0px 2px,   #000 0px 0px 2px,
                       #000 0px 0px 2px,   #000 0px 0px 2px,   #000 0px 0px 2px; 
      }
      
      .background-image {
          background-image: url(marris_background_frame.jpg);
          background-repeat: no-repeat;
          background-position: center;
      }
    </style>
    
  </head>
  <body class="d-flex flex-column h-100">

    <!-- Begin page content -->
    <header class="flex-shrink-0 bg-light py-5 mb-5 background-image">
      <div class="container text-center py-5">
        <!-- <h1 class="display-1 white-shadow text-white">Luke Marris</h1> -->
        <img src="marris_portrait.jpeg" class="rounded mx-auto d-block" alt="Luke Marris portrait" width="340" height="340">
      </div>
    </header>
    
    <main class="flex-shrink-0">
      <section class="container py-2">
        <h2>About Luke Marris</h2>
        <p>
          <img src="marris_portrait.jpeg" class="rounded mx-auto d-block" alt="Luke Marris portrait" width="300" height="300">
        </p>
        <p class="lead">
          Hello, I am Luke Marris, a Senior Research Engineer on the Multiagent team at <a href="https://www.deepmind.com/">DeepMind</a> and a PhD candidate at <a href="https://www.ucl.ac.uk/">University College London</a>.
          I am interested in training Deep Reinforcement Learning agents at scale in n-player general-sum environments, with a particular focus on equilibrium selection, equilibrium computation and Game Theory.
          Previously I obtained a first class Bachelors and Masters in Information Engineering at <a href="https://www.jesus.cam.ac.uk/">Jesus College</a>, <a href="https://www.cam.ac.uk/">University of Cambridge</a>.
        </p>
      </section>
      
      <section class="container  py-2">
        <h2>Papers and Publications</h1>
        <ul>
          <li class="mb-4">
            <h4>Turbocharging Solution Concepts: Solving NEs, CEs and CCEs with Neural Equilibrium Solvers</h4>
            <p>
              [<time datetime="2022">2022</time>] <strong>Luke Marris</strong>, Ian Gemp, Thomas Anthony, Andrea Tacchetti, Siqi Liu, Karl Tuyls
            </p>
            <p>
              <a href="https://arxiv.org/abs/2210.09257">arXiv</a> <a href="https://nips.cc/virtual/2022/poster/53003">Poster and Presentation</a> <a href="https://openreview.net/forum?id=RczPtvlaXPH">Open Review</a>
            </p>
            <p>
              Solution concepts such as Nash Equilibria, Correlated Equilibria, and Coarse Correlated Equilibria are useful components for many multiagent machine learning algorithms. Unfortunately, solving a normal-form game could take prohibitive or non-deterministic time to converge, and could fail. We introduce the Neural Equilibrium Solver which utilizes a special equivariant neural network architecture to approximately solve the space of all games of fixed shape, buying speed and determinism. We define a flexible equilibrium selection framework, that is capable of uniquely selecting an equilibrium that minimizes relative entropy, or maximizes welfare. The network is trained without needing to generate any supervised training data. We show remarkable zero-shot generalization to larger games. We argue that such a network is a powerful component for many possible multiagent algorithms.
            </p>
          </li>
          <li class="mb-4">
            <h4>Game Theoretic Rating in N-player general-sum games with Equilibria</h4>
            <p>
              [<time datetime="2022">2022</time>] <strong>Luke Marris</strong>, Marc Lanctot, Ian Gemp, Shayegan Omidshafiei, Stephen McAleer, Jerome Connor, Karl Tuyls, Thore Graepel
            </p>
            <p>
              <a href="https://arxiv.org/abs/2210.02205">arXiv</a> <a href="https://www.deepmind.com/publications/game-theoretic-rating-in-n-player-general-sum-games-with-equilibria">DeepMind</a>
            </p>
            <p>
              Rating strategies in a game is an important area of research in game theory and artificial intelligence, and can be applied to any real-world competitive or cooperative setting. Traditionally, only transitive dependencies between strategies have been used to rate strategies (e.g. Elo), however recent work has expanded ratings to utilize game theoretic solutions to better rate strategies in non-transitive games. This work generalizes these ideas and proposes novel algorithms suitable for N-player, general-sum rating of strategies in normal-form games according to the payoff rating system. This enables well-established solution concepts, such as equilibria, to be leveraged to efficiently rate strategies in games with complex strategic interactions, which arise in multiagent training and real-world interactions between many agents. We empirically validate our methods on real world normal-form data (Premier League) and multiagent reinforcement learning agent evaluation.
            </p>
          </li>
          <li class="mb-4">
            <h4>Developing, evaluating and scaling learning agents in multi-agent environments</h4>
            <p>
              [<time datetime="2022">2022</time>] Ian Gemp, Thomas Anthony, Yoram Bachrach, Avishkar Bhoopchand, Kalesha Bullard, Jerome Connor, Vibhavari Dasagi, Bart De Vylder, Edgar A Duéñez-Guzmán, Romuald Elie, Richard Everett, Daniel Hennes, Edward Hughes, Mina Khan, Marc Lanctot, Kate Larson, Guy Lever, Siqi Liu, <strong>Luke Marris</strong>, Kevin R McKee, Paul Muller, Julien Pérolat, Florian Strub, Andrea Tacchetti, Eugene Tarassov, Zhe Wang, Karl Tuyls
            </p>
            <p>
              <a href="https://content.iospress.com/articles/ai-communications/aic220113">AI Communications</a> <a href="https://arxiv.org/abs/2209.10958">arXiv</a>
            </p>
            <p>
              The Game Theory & Multi-Agent team at DeepMind studies several aspects of multi-agent learning ranging from computing approximations to fundamental concepts in game theory to simulating social dilemmas in rich spatial environments and training 3-d humanoids in difficult team coordination tasks. A signature aim of our group is to use the resources and expertise made available to us at DeepMind in deep reinforcement learning to explore multi-agent systems in complex environments and use these benchmarks to advance our understanding. Here, we summarise the recent work of our team and present a taxonomy that we feel highlights many important open challenges in multi-agent research.
            </p>
          </li>
          <li class="mb-4">
            <h4>Simplex Neural Population Learning: Any-Mixture Bayes-Optimality in Symmetric Zero-sum Games</h4>
            <p>
              [<time datetime="2022">2022</time>] S Liu, M Lanctot, <strong>L Marris</strong>, N Heess
            </p>
            <p>
              <a href="https://proceedings.mlr.press/v162/liu22h.html">ICML</a> <a href="https://arxiv.org/abs/2205.15879">arXiv</a> <a href="https://icml.cc/virtual/2022/poster/18415">Poster</a>
            </p>
            <p>
              Learning to play optimally against any mixture over a diverse set of strategies is of important practical interests in competitive games. In this paper, we propose simplex-NeuPL that satisfies two desiderata simultaneously: i) learning a population of strategically diverse basis policies, represented by a single conditional network; ii) using the same network, learn best-responses to any mixture over the simplex of basis policies. We show that the resulting conditional policies incorporate prior information about their opponents effectively, enabling near optimal returns against arbitrary mixture policies in a game with tractable best-responses. We verify that such policies behave Bayes-optimally under uncertainty and offer insights in using this flexibility at test time. Finally, we offer evidence that learning best-responses to any mixture policies is an effective auxiliary task for strategic exploration, which, by itself, can lead to more performant populations.
            </p>
          </li>
          <li class="mb-4">
            <h4>NeuPL: Neural Population Learning</h4>
            <p>
              [<time datetime="2022">2022</time>] S Liu, <strong>L Marris</strong>, D Hennes, J Merel, N Heess, T Graepel
            </p>
            <p>
              <a href="https://openreview.net/forum?id=MIX3fJkl_1">ICLR</a> <a href="https://arxiv.org/abs/2202.07415">arXiv</a> <a href="https://iclr.cc/media/iclr-2022/Slides/6950.pdf">Slides<a/> <a href="https://neupl.github.io/demo/">Website</a> <a href="https://iclr.cc/virtual/2022/poster/6950">Talk</a>
            </p>
            <p>
              Learning in strategy games (e.g. StarCraft, poker) requires the discovery of diverse policies. This is often achieved by iteratively training new policies against existing ones, growing a policy population that is robust to exploit. This iterative approach suffers from two issues in real-world games: a) under finite budget, approximate best-response operators at each iteration needs truncating, resulting in under-trained good-responses populating the population; b) repeated learning of basic skills at each iteration is wasteful and becomes intractable in the presence of increasingly strong opponents. In this work, we propose Neural Population Learning (NeuPL) as a solution to both issues. NeuPL offers convergence guarantees to a population of best-responses under mild assumptions. By representing a population of policies within a single conditional model, NeuPL enables transfer learning across policies. Empirically, we show the generality, improved performance and efficiency of NeuPL across several test domains. Most interestingly, we show that novel strategies become more accessible, not less, as the neural population expands.
            </p>
           </li>
          <li class="mb-4">
            <h4>Multi-agent training beyond zero-sum with correlated equilibrium meta-solvers</h4>
            <p>
              [<time datetime="2021">2021</time>] <strong>L Marris</strong>, P Muller, M Lanctot, K Tuyls, T Graepel
            </p>
            <p>
              <a href="https://proceedings.mlr.press/v139/marris21a">ICML</a> <a href="https://arxiv.org/abs/2106.09435">arXiv</a> <a href="https://icml.cc/media/icml-2021/Slides/10003.pdf">Slides</a> <a href="https://papertalk.org/papertalks/32703">Talk</a> <a href="https://paperswithcode.com/paper/multi-agent-training-beyond-zero-sum-with">Code</a> <a href="https://www.deepmind.com/publications/multi-agent-training-beyond-zero-sum-with-correlated-equilibrium-meta-solvers">DeepMind</a>
            </p>
            <p>
              Two-player, constant-sum games are well studied in the literature, but there has been limited progress outside of this setting. We propose Joint Policy-Space Response Oracles (JPSRO), an algorithm for training agents in n-player, general-sum extensive form games, which provably converges to an equilibrium. We further suggest correlated equilibria (CE) as promising meta-solvers, and propose a novel solution concept Maximum Gini Correlated Equilibrium (MGCE), a principled and computationally efficient family of solutions for solving the correlated equilibrium selection problem. We conduct several experiments using CE meta-solvers for JPSRO and demonstrate convergence on n-player, general-sum games.
            </p>
          </li>
          <li class="mb-4">
            <h4>From motor control to team play in simulated humanoid football</h4>            
            <p>
              [<time datetime="2021">2021</time>] Siqi Liu, Guy Lever, Zhe Wang, Josh Merel, SM Eslami, Daniel Hennes, Wojciech M Czarnecki, Yuval Tassa, Shayegan Omidshafiei, Abbas Abdolmaleki, Noah Y Siegel, Leonard Hasenclever, <strong>Luke Marris</strong>, Saran Tunyasuvunakool, H Francis Song, Markus Wulfmeier, Paul Muller, Tuomas Haarnoja, Brendan D Tracey, Karl Tuyls, Thore Graepel, Nicolas Heess
            </p>
            <p>
              <a href="https://www.science.org/doi/10.1126/scirobotics.abo0235">Science Robotics</a> <a href="https://arxiv.org/abs/2105.12196">arXiv</a> <a href="https://youtu.be/KHMwq9pv7mg">Video</a>
            </p>
            <p>
              Intelligent behaviour in the physical world exhibits structure at multiple spatial and temporal scales. Although movements are ultimately executed at the level of instantaneous muscle tensions or joint torques, they must be selected to serve goals defined on much longer timescales, and in terms of relations that extend far beyond the body itself, ultimately involving coordination with other agents. Recent research in artificial intelligence has shown the promise of learning-based approaches to the respective problems of complex movement, longer-term planning and multi-agent coordination. However, there is limited research aimed at their integration. We study this problem by training teams of physically simulated humanoid avatars to play football in a realistic virtual environment. We develop a method that combines imitation learning, single- and multi-agent reinforcement learning and population-based training, and makes use of transferable representations of behaviour for decision making at different levels of abstraction. In a sequence of stages, players first learn to control a fully articulated body to perform realistic, human-like movements such as running and turning; they then acquire mid-level football skills such as dribbling and shooting; finally, they develop awareness of others and play as a team, bridging the gap between low-level motor control at a timescale of milliseconds, and coordinated goal-directed behaviour as a team at the timescale of tens of seconds. We investigate the emergence of behaviours at different levels of abstraction, as well as the representations that underlie these behaviours using several analysis techniques, including statistics from real-world sports analytics. Our work constitutes a complete demonstration of integrated decision-making at multiple scales in a physically embodied multi-agent setting.
            </p>
          </li>
          <li class="mb-4">
            <h4>Backpropagation and the brain</h4>
            <p>
              [<time datetime="2020">2020</time>] Timothy P Lillicrap, Adam Santoro, <strong>Luke Marris</strong>, Colin J Akerman, Geoffrey Hinton
            </p>
            <p>
              <a href="https://www.nature.com/articles/s41583-020-0277-3">Nature Reviews Neuroscience</a> <a href="https://ora.ox.ac.uk/objects/uuid:862189c1-0088-4f78-b17a-2748c2019209/download_file?safe_filename=Lillicrap_v6_2020.pdf&type_of_work=Journal+article">Open</a>
            </p>
            <p>
              During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.
            </p>
          </li>
          <li class="mb-4">
            <h4>A generalized training approach for multiagent learning</h4>
            <p>
              [<time datetime="2019">2019</time>] Paul Muller, Shayegan Omidshafiei, Mark Rowland, Karl Tuyls, Julien Perolat, Siqi Liu, Daniel Hennes, <strong>Luke Marris</strong>, Marc Lanctot, Edward Hughes, Zhe Wang, Guy Lever, Nicolas Heess, Thore Graepel, Remi Munos
            </p>
            <p>
              <a href="https://iclr.cc/virtual_2020/poster_Bkl5kxrKDr.html">ICLR</a> <a href="https://arxiv.org/abs/1909.12823">ArXiv</a>
            </p>
            <p>
              This paper investigates a population-based training regime based on game-theoretic principles called Policy-Spaced Response Oracles (PSRO). PSRO is general in the sense that it (1) encompasses well-known algorithms such as fictitious play and double oracle as special cases, and (2) in principle applies to general-sum, many-player games. Despite this, prior studies of PSRO have been focused on two-player zero-sum games, a regime wherein Nash equilibria are tractably computable. In moving from two-player zero-sum games to more general settings, computation of Nash equilibria quickly becomes infeasible. Here, we extend the theoretical underpinnings of PSRO by considering an alternative solution concept, α-Rank, which is unique (thus faces no equilibrium selection issues, unlike Nash) and applies readily to general-sum, many-player settings. We establish convergence guarantees in several games classes, and identify links between Nash equilibria and α-Rank. We demonstrate the competitive performance of α-Rank-based PSRO against an exact Nash solver-based PSRO in 2-player Kuhn and Leduc Poker. We then go beyond the reach of prior PSRO applications by considering 3- to 5-player poker games, yielding instances where α-Rank achieves faster convergence than approximate Nash solvers, thus establishing it as a favorable general games solver. We also carry out an initial empirical validation in MuJoCo soccer, illustrating the feasibility of the proposed approach in another complex domain.
            </p>
          </li>
          <li class="mb-4">
            <h4>Human-level performance in 3D multiplayer games with population-based reinforcement learning</h4>
            <p>
              [<time datetime="2019">2019</time>] Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, <strong>Luke Marris</strong>, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, Thore Graepel
            </p>
            <p>
              <a href="https://www.science.org/doi/10.1126/science.aau6249">Science</a> <a href="https://arxiv.org/abs/1807.01281">arXiv</a> <a href="https://www.deepmind.com/blog/capture-the-flag-the-emergence-of-complex-cooperative-agents">Blog</a>
            </p>
            <p>
              Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.
            </p>
          </li>
          <li class="mb-4">
            <h4>Assessing the scalability of biologically-motivated deep learning algorithms and architectures</h4>
            <p>
              [<time datetime="2018">2018</time>] S Bartunov, A Santoro, B Richards, <strong>L Marris</strong>, GE Hinton, T Lillicrap
            </p>
            <p>
              <a href="https://papers.nips.cc/paper/2018/hash/63c3ddcc7b23daa1e42dc41f9a44a873-Abstract.html">NeurIPS</a> <a href="https://arxiv.org/abs/1807.04587">arXiv</a>
            </p>
            <p>
              The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.
            </p>
          </li>
        </ul>
      </section>
    </main>

    <footer class="footer mt-auto py-3 bg-light">
      <div class="container">
        <span class="text-muted text-center">
          <address>🏡 London</address>
          <p>
            Follow me on <a href="https://twitter.com/MarrisLuke">twitter</a>.
            Connect on <a href="https://www.linkedin.com/in/lukemarris/">LinkedIn</a>.
            View my <a href="https://scholar.google.com/citations?user=dvTeSX4AAAAJ">Google Scholar</a>.
          </p>
          <p>Made with ♣♦♥♠</p>
        </span>
      </div>
    </footer>

    <!-- JavaScript Bundle with Popper -->
    <script
        src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-OERcA2EqjJCMA+/3y+gxIOqMEjwtxJY7qPCqsdltbNJuaOe923+mo//f6V8Qbsw3"
        crossorigin="anonymous">
    </script>
      
  </body>
</html>
